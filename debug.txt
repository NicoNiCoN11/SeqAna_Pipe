BUG EXPLANATION AND FIX
========================

WHAT WENT WRONG:
----------------
Your pipeline only aligned ONE sample (rep1_0) instead of all 7 samples.

ROOT CAUSE:
-----------
In Nextflow DSL2, channels can only be consumed ONCE. Your pipeline did this:

  1. Created reads_ch from samplesheet (7 samples)
  2. FASTQC consumed reads_ch (all 7 samples) ✓
  3. STAR_ALIGN tried to use reads_ch again → EMPTY! (or 1 leftover item)

This is why you see:
  - FASTQC (rep2_0) | 7 of 7 ✓  ← Processed all 7
  - STAR_ALIGN (rep1_0) | 1 of 1 ✓  ← Only processed 1!


THE FIX:
--------
Use .multiMap() to duplicate the channel into separate streams:

  OLD CODE (lines 163-190):
  --------------------------
  Channel.fromPath(...)
      .splitCsv(...)
      .map { ... }
      .set { reads_ch }
  
  FASTQC(reads_ch)         # ← Consumes entire channel
  STAR_ALIGN(reads_ch, ...) # ← Gets nothing!


  NEW CODE (lines 163-210):
  --------------------------
  def samplesheet_ch = Channel.fromPath(...)
      .splitCsv(...)
      .map { ... }
  
  samplesheet_ch.multiMap { sample_id, reads ->
      fastqc: tuple(sample_id, reads)  # ← Separate stream
      star: tuple(sample_id, reads)     # ← Separate stream
  }.set { reads_ch }
  
  FASTQC(reads_ch.fastqc)      # ← Uses fastqc stream
  STAR_ALIGN(reads_ch.star, ...) # ← Uses star stream


HOW TO RESUME:
--------------
You can resume from where you left off and avoid re-running FASTQC and STAR_INDEX!

1. Replace main.nf with main_fixed.nf:
   
   mv main.nf main_old.nf
   mv main_fixed.nf main.nf


2. Resume the pipeline with -resume flag:
   
   nextflow run main.nf \
     --samplesheet /home/jiguo/SeqAna_Pipe/samplesheet.csv \
     --genome /home/jiguo/data/data/ref_genome/GRCh38.primary_assembly.genome.fa \
     --gtf /home/jiguo/data/data/ref_genome/gencode.v47.primary_assembly.annotation.gtf \
     --star_index /home/jiguo/data/data/outdir_rnaseq/star_index/star_index \
     --outdir /home/jiguo/data/data/outdir_rnaseq \
     -resume


   Note: Using --star_index to reuse the already-generated index!


WHAT WILL HAPPEN:
-----------------
With -resume, Nextflow will:
  ✓ SKIP: FASTQC (already done, cached)
  ✓ SKIP: STAR_INDEX (already done, cached)
  ✓ SKIP: STAR_ALIGN for rep1_0 (already done)
  ⚡ RUN: STAR_ALIGN for rep1_1, rep2_0, rep2_1, rep3_0, rep3_1, control (6 new alignments)
  ⚡ RE-RUN: FEATURECOUNTS (needs all 7 BAMs, will collect them)
  ⚡ RE-RUN: MULTIQC (aggregates all results)

Estimated time: ~2-3 hours (only alignments, no indexing)


WHAT YOU SHOULD SEE:
--------------------
After the fixed run, you should have in aligned/:
  - rep1_0.Aligned.sortedByCoord.out.bam (already there)
  - rep1_0.Aligned.sortedByCoord.out.bam.bai
  - rep1_1.Aligned.sortedByCoord.out.bam (NEW)
  - rep1_1.Aligned.sortedByCoord.out.bam.bai (NEW)
  - rep2_0.Aligned.sortedByCoord.out.bam (NEW)
  - rep2_0.Aligned.sortedByCoord.out.bam.bai (NEW)
  - rep2_1.Aligned.sortedByCoord.out.bam (NEW)
  - rep2_1.Aligned.sortedByCoord.out.bam.bai (NEW)
  - rep3_0.Aligned.sortedByCoord.out.bam (NEW)
  - rep3_0.Aligned.sortedByCoord.out.bam.bai (NEW)
  - rep3_1.Aligned.sortedByCoord.out.bam (NEW)
  - rep3_1.Aligned.sortedByCoord.out.bam.bai (NEW)
  - control.Aligned.sortedByCoord.out.bam (NEW)
  - control.Aligned.sortedByCoord.out.bam.bai (NEW)

Total: 14 files (7 BAM + 7 BAI)


VERIFICATION:
-------------
After completion, check:
  
  # Count BAM files
  ls -1 /home/jiguo/data/data/outdir_rnaseq/aligned/*.bam | wc -l
  # Should show: 7
  
  # Check counts matrix
  head -1 /home/jiguo/data/data/outdir_rnaseq/counts/counts.txt
  # Should show all 7 sample columns
  
  # Open MultiQC report
  firefox /home/jiguo/data/data/outdir_rnaseq/multiqc/multiqc_report.html


IMPORTANT NOTES:
----------------
- The -resume flag uses Nextflow's work/ directory cache
- If you deleted work/, you'll have to re-run everything
- The fixed pipeline will work correctly for future runs
- This is a common Nextflow DSL2 pitfall!


WHY LOCAL EXECUTOR?
-------------------
Your log shows "executor > local" instead of SLURM.
This means you ran it on the login node, which is not ideal.

For future runs, use the submit_pipeline.sh I provided earlier
to run via SLURM with proper resource allocation.